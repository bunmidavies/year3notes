# chip multiprocessors

### course division
- Part A - Introduction 
- Part B - Multiprocessor Programming and Hardware Support
- Part C - Other Approaches
### reading
- [[Computer-Architecture-A-Quantitative-Approach.pdf|Computer Architecture - A Quantitative Approach]]

### random things i'm not sure where to put
- [[x86]] and [[x86-64]]
### assessment
- Lab 1: Delivering speedup for vector addition - 23 feb
- Lab 2: Synchronisation: the dining philosophers problem - 15 mar
- Lab 3: Relaxation (to solve Poisson's equation) - 26 apr

> [!note] week 1
> - [intro 1 slides](https://olivierpierre.github.io/comp35112/lectures/01a-introduction-1/#1)
> - intro live lecture: logistics, why multiprocessors? transistor count timeline - ==architectural approaches to increasing single processor clock speed have been exhausted==, moores law (mostly due to transistor size reduction)
>   <br>
>   
> - Lecture 1: Introduction
> 	- [[dennard scaling]] - single core performance hits wall, resort to multicore, resulting in parallelism to be exploited
> 	- [[instruction-level parallelism (ILP)]] - low amount of parallelism to be extracted
> 	- [[thread-level parallelism (TLP)]] - higher amount of parallelism to be extracted
> 	- [[data-level parallelism (DLP)]]
> 	- [[chip multiprocessor considerations]] - hardware/software divide
> 	- [reading: 'The Free Lunch Is Over', Herb Sutter, 2005](http://www.gotw.ca/publications/concurrency-ddj.htm)
> 
>   <br>
> - Lecture 2: The World of Parallelism
> 	- trends of core count so far: seemingly doubling every year - at this rate, by 2030 even mid-range smartphones may have 100+ general purpose cores
> 	- [[flynn's taxonomy]] - a way to classify parallel computer architectures
> 	- [[on chip interconnects]] - between cores and between cores and memory:
> 		- grid interconnect
> 		- torus interconnect
> 		- bus interconnect
> 	- [[shared memory vs distributed memory]] - in general, implementations must balance between ease of programming and hardware efficiency

> [!note] Week 2
> - Lecture 3: Parallel Programming Using Shared Memory
> 	- [slides](https://olivierpierre.github.io/comp35112/lectures/03-shared-memory-programming/#1)
> 	- [oracle multithreaded programming guide](https://docs.oracle.com/cd/E53394_01/pdf/E54803.pdf)
> 	- [[OS processes + process control blocks (PCB)]]
> 	- [[threads]] + [[POSIX threads (Pthread)]]
> 	- [[threads in java]]
> 	- important point: ==OS scheduling is non-deterministic==
> 	- recap on [[data-level parallelism (DLP)]]
> 	- [[explicit vs implicit parallelism]]
> 	- recap on [[shared memory vs distributed memory|shared memory]]
> 
> - Lecture 4: Shared Memory Multiprocessors
> 	- [slides](https://olivierpierre.github.io/comp35112/lectures/04-shared-memory-multiprocessors/#1)
> 	- [[multiprocessor structure]]
> 	- [[cache coherence]]

> [!note] Week 3
> - Lecture 5: Cache Coherence
> 	- [slides](https://olivierpierre.github.io/comp35112/lectures/05-cache-coherence/)
> 	- [[bus snooping]]
> 	- [[MSI cache states]]
> 	- [[write-invalidate vs write-update]]
> - Lecture 6: MESI/MOESI
> 	- [[MESI protocol]]
> 	- [[MOESI protocol]]
> - Lecture 7: Directory-Based Cache Coherence
> 	- [[directory based protocols]]
> 	- [[NUMA systems]]

> [!note] Week 4
> - Lecture 8A: Synchronisation 1 (Barriers + Locks)
> 	- [slides](https://olivierpierre.github.io/comp35112/lectures/08a-locks-barriers/#1)
> 	- [[the need for synchronisation mechanisms]]
> 	- [[barriers]]
> 	- [[locks]] + [[pthread mutex]]
> - Lecture 8B: Synchronisation 2 (Condition Variables)
> 	- [[condition variables]]
> - Lecture 9: More about Locks
> 	- [[deadlocks]]
> 	- [[lost wakeup issue]]
> 	- [[locking granularity]]
> 	- [[reentrant lock]]
> 	- [[semaphores]]
> 	- [[spinlocks]]
> 	- [[read-write locks]]

> [!note] Week 5
> - Lecture 10:
> 	- synchronisation requires support from hardware to ensure that critical sections are executed ==atomically== - atomic RMW instructions exist, but are costly+difficult to implement on RISC
> 	- [[binary semaphore]]
> 	- [[test-and-set instruction]]
> 	- [[test-and-test-and-set]]
> 	- [[fetch-and-add, compare-and-swap]]
> 	- [[lock-free data structures]]
> - Lecture 11: Load-Linked and Store-Conditional
> 	- LL/SC used in RISC processors: ARM, IBM Power etc - work like regular load/stores but have additional effects on CPU state
> 	- [[load-linked]]
> 	- [[store-conditional]]
> 	- [[semaphore with LL+SC + other uses]]

> [!note] Week 6
> - Lecture 12A: Operating System Support for Multithreading
> 	- [[thread creation]]
> 	- [[lock implementations]]
> 	- [[futex]]
> 	- [[concurrency in the linux kernel]]
> - Lecture 12B: Read Copy Update (RCU)
> 	- [[read-copy-update]]
> - Lecture 13: MPI
> 	- as we already know, multithreading is not a solution for all issues, as 1. shared memory limits us to a single machine 2. extra effort is required to divide parallel work. Two standard parallel programming frameworks exist for this - MPI and OpenMP
> 	- [[message passing interface (MPI)]]
> 	- [[sending+receiving with MPI]]
> 	- [[collective operations with MPI]]
> 	- [[MPI modes of communication]]
> 	- [[one-sided communication in MPI]]

> [!Note] Week 7
> - Lecture 14: High Level Parallel Programming
> 	- *A*
> 	- recap + pitfalls of [[POSIX threads (Pthread)]]
> 	- std::thread offers c++ syntactic sugar to make pthreads easier to use, allowing you to use fewer void pointers
> 	- [[lock guards]]
> 	- [[scoped locks]]
> 	- [[std atomic|std::atomic]]
> 	- [[std barrier|std::barrier]]
> 	- *B*
> 	- [[parallel stl policies]]
> 	- [[std future|std::future]]
> 	- *C*
> 	- [[levels of parallelism abstraction]]
> - Lecture 15: OpenMP
> 	- [[OpenMP]]
> 

> [!note] Week 8
> - Lecture 16: GPUs and GPU Programming
> 	- *A*
> 	- CPUs vs GPUs, Many-Core vs Multi-Core - the main point is that GPUs have similar control units, designed for ==high throughput==
> 	- GPUs = SIMT ([[flynn's taxonomy]])
> 	- *B*
> 	- [[programming for GPUs]]
> 	- [[CUDA programming language]]
> 	- *C*
> 	- [[CUDA thread hierarchy]]
> 	- [[future of CUDA]]
> 	- *D*
> 	- [[OpenCL]]
> 	- [[performance portable libraries]]

> [!Note] Week 9
> - Lecture 17: Heterogeneous CMPs 
> 	- [[general purpose accelerators]]
> 	- [[asymmetric multicores]]

> [!Note] Week 10
> - Lecture 18: Speculation
> 	- [[thread-level speculation (TLS)]]
> 	- [[polyhedral transformations]]
> - Lecture 19: Transactional Memory
> 	- [[transactional memory]]
> 	- [[versioning and validation]]