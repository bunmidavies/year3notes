# chip multiprocessors

### course division
- Part A - Introduction 
- Part B - Multiprocessor Programming and Hardware Support
- Part C - Other Approaches
### reading
- [[Computer-Architecture-A-Quantitative-Approach.pdf|Computer Architecture - A Quantitative Approach]]
### assessment
- Lab 1: Delivering speedup for vector addition - 23 feb
- Lab 2: Synchronisation: the dining philosophers problem - 15 mar
- Lab 3: Relaxation (to solve Poisson's equation) - 26 apr

> [!note] week 1
> - [intro 1 slides](https://olivierpierre.github.io/comp35112/lectures/01a-introduction-1/#1)
> - intro live lecture: logistics, why multiprocessors? transistor count timeline - ==architectural approaches to increasing single processor clock speed have been exhausted==, moores law (mostly due to transistor size reduction)
>   <br>
>   
> - Lecture 1: Introduction
> 	- [[dennard scaling]] - single core performance hits wall, resort to multicore, resulting in parallelism to be exploited
> 	- [[instruction-level parallelism (ILP)]] - low amount of parallelism to be extracted
> 	- [[thread-level parallelism (TLP)]] - higher amount of parallelism to be extracted
> 	- [[data-level parallelism (DLP)]]
> 	- [[chip multiprocessor considerations]] - hardware/software divide
> 	- [reading: 'The Free Lunch Is Over', Herb Sutter, 2005](http://www.gotw.ca/publications/concurrency-ddj.htm)
> 
>   <br>
> - Lecture 2: The World of Parallelism
> 	- trends of core count so far: seemingly doubling every year - at this rate, by 2030 even mid-range smartphones may have 100+ general purpose cores
> 	- [[flynn's taxonomy]] - a way to classify parallel computer architectures
> 	- [[on chip interconnects]] - between cores and between cores and memory:
> 		- grid interconnect
> 		- torus interconnect
> 		- bus interconnect
> 	- [[shared memory vs distributed memory]] - in general, implementations must balance between ease of programming and hardware efficiency

> [!note] Week 2
> - Lecture 3: Parallel Programming Using Shared Memory
> 	- [slides](https://olivierpierre.github.io/comp35112/lectures/03-shared-memory-programming/#1)
> 	- [oracle multithreaded programming guide](https://docs.oracle.com/cd/E53394_01/pdf/E54803.pdf)
> 	- [[OS processes + process control blocks (PCB)]]
> 	- [[threads]] + [[POSIX threads (Pthread)]]
> 	- [[threads in java]]
> 	- important point: ==OS scheduling is non-deterministic==
> 	- recap on [[data-level parallelism (DLP)]]
> 	- [[explicit vs implicit parallelism]]
> 	- recap on [[shared memory vs distributed memory|shared memory]]
> 
> - Lecture 4: Shared Memory Multiprocessors
> 	- [slides](https://olivierpierre.github.io/comp35112/lectures/04-shared-memory-multiprocessors/#1)
> 	- [[multiprocessor structure]]
> 	- [[cache coherence]]

> [!note] Week 3
> - Lecture 5: Cache Coherence
> 	- [slides](https://olivierpierre.github.io/comp35112/lectures/05-cache-coherence/)
> 	- [[bus snooping]]
> 	- [[MSI cache states]]
> 	- [[write-invalidate vs write-update]]

> [!note] Week 4
> - Lecture 8A: Synchronisation 1
> 	- [slides](https://olivierpierre.github.io/comp35112/lectures/08a-locks-barriers/#1)
> 	- [[the need for synchronisation mechanisms]]
> 	- [[barriers]]
> 	- [[locks]] + [[pthread mutex]]