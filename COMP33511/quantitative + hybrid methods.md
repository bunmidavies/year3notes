[[COMP33511]]

- while [[qualitative evaluation methods]] are typically used to build a body of evidence both deep and narrow in extent and scope, quantitative and hybrid methods can build upon any hypotheses generated, or extend any understanding built through qualitative methods
- certain facets need to be carefully considered with quantitative measures - take questionnaires for instance, which can be useful for collecting 'confirmatory information' of previous hypotheses. Many assumptions can already be made just based off the questions that participants are asked - this can introduce bias, ultimately skewing results
- ==hybrid methods aka triangulation or mixed methods== exist in order to try and eliminate any bias, by avoiding the constriction of any single method through combining multiple quantitative methods to confirm qualitative findings

# card sorting + triadic elicitation
- in card sorting, the participant is given many cards each displaying the name of a concept (or an image, or screenshot) - the participant must repeatedly sort the cards into piles such that the cards in each pile have something in common
- by voicing what each pile has in common, or the difference between each pile, implicit knowledge about things on the card is vocalised by the participant
- triadic elicitation is often used along with card sorting techniques - the participant is asked about similarities/differences between 3 randomly chosen concepts, as well as ==which 2 cards are the most similar== - attributes that are not immediately/easily articulated can be elicited from the participant

# socio/unobtrusive methods
- the same idea as [[qualitative evaluation methods#unobtrusive methods]] - UX as a whole can be unobtrusively measured, directly or indirectly; individually or collectively. Through digital methods, appropriate metrics can be extracted in order to allow for constant improvement of products and services
- PULSE analytics - Page views, Uptime, Latency, Seven-day active users, Earnings - can be derived from standard quantitative data to provide useful insights
- HEART analytics - Happiness, Engagement, Adoption, Retention, Task success - requires more social networking + user monitoring in comparison to PULSE, but once acquired would allow favourable experiences to be much more easily detected

# longitudinal analysis
- longitudinal analysis essentially means observing and recording the same measurements/variables over an extended period of time - increased intervals enable the consolidation of declarative knowledge in long-term memory
	- ==think-aloud== - more or less an observational process, the methodology is less concerned with measuring task completion, but more so with recording associated verbalisations of participants as they progress through a task, describing how they're feeling and what they think they need to do. The main concern with this as previously discussed, is that verbalisation/thinking aloud can implicitly influence how a participant interacts with the system, so this should be used in a more qualitative sense when other quantitative methods lack in such particular measures
	- ==co-operative evaluation+participatory design== - in short, participants are encouraged to criticise a system in an attempt to extract real requirements - in some cases this can mean that the system design is created before co-operative aspects have begun. Co-operative evaluation is strongly related to [[user centred design]], emphasising the involvement of users in the design/evaluation of interactive systems. The two are explained better below

# cooperative evaluation / participatory design similarities+differences
- cooperative evaluation and participatory design are related approaches that share several similarities including:
	1. user participation within practices
	2. collaborative approaches involving open communication
	3. user empowerment, giving users a voice
	4. iterative/agile processes
	5. contextual understanding
	6. user-centric design, focusing on users needs/preferences
- while these similarities exist, cooperative evaluation primarily exists for evaluating and refining an existing system
- meanwhile, participatory design is concerned with involving users in the entire design process from ideation to implementation
- in short, ==cooperative = existing system, participatory = design from scratch==
- neither of these are fast solutions - the UX specialist remains as a facilitator, as the participants views and thoughts are the key factor in the design process