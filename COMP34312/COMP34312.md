# mathematical topics in machine learning
Are <u>bigger</u> models always <u>better</u> models?

- [[NotesPack.pdf|Complete Notes]]

> [!example] various handwritten notes (probably typed up at some point), or asides
> [[how can the true distribution have noise]]

> [!abstract] Week 1 - COMP24112 Recap
> - welcome lecture
> - [[Lec1-Recap.pdf|Recap Slides]]
> - this module provides the mathematical+conceptual tools to discuss/debate the question
> - supervised learning
> - k-nearest neighbours
> - classification + simplex
> - [understanding KL divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM) -> [understanding cross entropy loss](https://www.youtube.com/watch?v=Pwgpl9mKars)

> [!abstract] Week 2 - Empirical Risk Minimization
> - [[Lec2-ERM.pdf|Slides]]
> - [[statistical learning theory]]
> - [[model family]]
> - [[IID assumption]]
> - [[empirical risk + empirical risk minimizer (ERM)]]
> - [[population risk]]
> - [[bayes model]]
> - [[excess risk]]
> - [[approximation-estimation-optimisation]]

> [!abstract] Week 3 - Generalisation Bounds
> - [[Lec3-Hoeffding.pdf|Slides]]
> - [[law of large numbers]]
> - [[hoeffding's inequality]]

> [!abstract] Week 4 - Bias-Variance decompositions
> - [[Lec4-BV.pdf|Slides]]
> - [[dealing with high bias + high variance]]
> - [[double descent]]

> [!abstract] Week 5 - Ensemble Learning
> - [[ambiguity decomposition]]
> - [[KL divergence]]
> - [[majority voting]]
> - [[independence-]]
> - [[sequential ensemble construction]]
> - [[parallel ensemble construction]]
> - [[bootstrapping]]
> - [[random forests algorithm]]
> - [[boosting]] + [[adaboost]]

> [!abstract] Week 6 - Ensemble Theory
> - [[Lec6-Diversity.pdf|Slides]] + [[unified-theory-diversity.pdf|Reading]]
> - [[diversity]]